{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects.numpy2ri\n",
    "from stldecompose import decompose\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "stats = importr('stats')\n",
    "stl=stats.stl\n",
    "ts =stats.ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 123)\n",
      "(72, 90)\n"
     ]
    }
   ],
   "source": [
    "data= pd.read_csv(\"Data/FromMohammed/cif_dataset_complete.csv\",header=None)\n",
    "predictions = pd.read_csv(\"Data/FromMohammed/theta_25_horg.csv\",index_col=0,skiprows = [1])\n",
    "print(data.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "#     depth=data.shape[2]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size+1-horizon):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length-window_size+1-horizon,horizon])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "#     print(str(extra))\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "#     print(data)\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],extra)\n",
    "    data = np.append(data,data_app)    \n",
    "#     data = np.append(data,np.zeros([horizon-extra]))\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(val,preds,horizon):\n",
    "    temp = np.abs(val-preds)\n",
    "    temp1=np.abs(val)+np.abs(preds)\n",
    "    \n",
    "    smape = 200/horizon*np.sum(temp/temp1)\n",
    "\n",
    "    return smape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def smape_nzr(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200.0 * np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder - All Theta rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_length = predictions.shape[0]\n",
    "\n",
    "horizon=6\n",
    "window_size=6\n",
    "theta_all=np.zeros(0)\n",
    "for i in range(theta_length):\n",
    "    current_pred= np.asarray(predictions.iloc[i].dropna().values,dtype=float)\n",
    "    theta_all=np.concatenate((theta_all,current_pred),axis=0)\n",
    "theta_input = make_k_input(theta_all,window_size,horizon)\n",
    "normalized_theta_input=np.zeros(theta_input.shape)\n",
    "for j in range(theta_input.shape[0]):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(theta_input[j])\n",
    "    normalized_theta_input[j]= scaler.transform(theta_input[j])\n",
    "#######################################################################################\n",
    "norm_reshaped_input=normalized_theta_input.reshape(normalized_theta_input.shape[0],6)\n",
    "ncol = normalized_theta_input.shape[1] #6\n",
    "\n",
    "encoding_dim = 4\n",
    "input_dim = Input(shape = (ncol, ))\n",
    "# Encoder Layers\n",
    "encoded1 = Dense(5, activation = 'relu')(input_dim)\n",
    "encoded2 = Dense(encoding_dim, activation = 'relu')(encoded1)\n",
    "# Decoder Layers\n",
    "decoded1 = Dense(5, activation = 'relu')(encoded2)\n",
    "decoded2 = Dense(ncol, activation = 'sigmoid')(decoded1)\n",
    "# Combine Encoder and Deocder layers\n",
    "autoencoder = Model(inputs = input_dim, outputs = decoded2)\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "autoencoder.fit(norm_reshaped_input, norm_reshaped_input, epochs = 500, batch_size = 32, shuffle = False)\n",
    "encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "encoded_input = Input(shape = (encoding_dim, ))\n",
    "auto_out = np.array(encoder.predict(norm_reshaped_input))\n",
    "\n",
    "new_theta_input=auto_out.reshape(auto_out.shape[0],auto_out.shape[1],1)\n",
    "autoencoder.save('MyModelCheckpoint/autoencoder_model.h5')\n",
    "encoder.save('MyModelCheckpoint/encoder_model.h5')\n",
    "np.savetxt('MyModelCheckpoint/new_theta_input.csv',auto_out, fmt='%1.3f',delimiter=',')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case1: Using only Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 17:56:38.221055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [52:49<00:00, 47.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-06 18:49:27.265232\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "13.374298976722933\n",
      "SMAPE2 (NZR approach)\n",
      "13.374298976722933\n",
      "MSE1:\n",
      "2883138816411.7314\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape1=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "####################################################################################\n",
    "##############creating train, validation and test sets from Data####################\n",
    "####################################################################################\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "        \n",
    "        \n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "################    Creating inputs for Neural Network      ########################\n",
    "####################################################################################\n",
    "        #train_input = np.append(x_train,train_pred,axis=1)\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred,axis=1)\n",
    "        test_input = x_test\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        #input_data= Input(batch_shape=(None,window_size+horizon,1),name='input_data')\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF01.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "        model.load_weights('MyModelCheckpoint/CIF01.h5')\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape1[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "    \n",
    "np.savetxt('MyModelCheckpoint/prediction01.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 , 3 , 4 , 5 : Using Autoencoder on Expert Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Autoencoder for Expert Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_out=pd.read_csv(\"MyModelCheckpoint/new_theta_input.csv\",header=None)\n",
    "encoding_dim2 = 32\n",
    "input_dim2 = Input(shape = (4, ))\n",
    "# Encoder Layers\n",
    "encoded12 = Dense(8, activation = 'relu')(input_dim2)\n",
    "encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "# Decoder Layers\n",
    "decoded12 = Dense(16, activation = 'relu')(encoded32)\n",
    "decoded22 = Dense(8, activation = 'relu')(decoded12)\n",
    "decoded32 = Dense(4, activation = 'sigmoid')(decoded22)\n",
    "\n",
    "# Combine Encoder and Deocder layers\n",
    "autoencoder2 = Model(inputs = input_dim2, outputs = decoded32)\n",
    "autoencoder2.compile(optimizer = 'adam', loss = 'mse')\n",
    "autoencoder2.fit(auto_out, auto_out, epochs = 100, batch_size = 32, shuffle = False)\n",
    "\n",
    "encoder2 = Model(inputs = input_dim2, outputs = encoded32)\n",
    "encoded_input2 = Input(shape = (encoding_dim2, ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Adding Expert Knowledge to layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape2=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=add([branch_0,input_pred])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        #branch_1=add([branch_1,input_pred])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        #branch_2=add([branch_2,input_pred])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF02.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_final},y_train,validation_data=[[val_input,val_pred_final],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF02.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_final})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape2[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction02.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.177486495179359\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape2)/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Case 3: Adding Expert Knowledge to layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape3=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        #branch_0=add([branch_0,input_pred])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_1=add([branch_1,input_pred])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        #branch_2=add([branch_2,input_pred])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF03.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_final},y_train,validation_data=[[val_input,val_pred_final],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF03.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_final})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape3[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction03.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.712916680754685\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape3)/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4: Adding Expert Knowledge to layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape4=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        #branch_0=add([branch_0,input_pred])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        #branch_1=add([branch_1,input_pred])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_2=add([branch_2,input_pred])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF04.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_final},y_train,validation_data=[[val_input,val_pred_final],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF04.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_final})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape4[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction04.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.894485371826107\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape4)/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5: Adding Expert Knowledge to All 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape5=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=add([branch_0,input_pred])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_1=add([branch_1,input_pred])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_2=add([branch_2,input_pred])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF05.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_final},y_train,validation_data=[[val_input,val_pred_final],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF05.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_final})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape5[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction05.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.26664535383255\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape5)/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: Considering Expert Knowledge as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape6=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "        \n",
    "        ############################################################################################################\n",
    "        train_input = np.append(x_train,train_pred_final.reshape(train_pred_final.shape[0],train_pred_final.shape[1],1),axis=1)\n",
    "        val_input = np.append(x_val,val_pred_final.reshape(val_pred_final.shape[0],val_pred_final.shape[1],1),axis=1)\n",
    "        test_input = np.append(x_test,test_pred_final.reshape(test_pred_final.shape[0],test_pred_final.shape[1],1),axis=1)\n",
    "        \n",
    "        \n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size+32,1),name='input_data')\n",
    "        #input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF06.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF06.h5')\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape6[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction06.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.35650816480022\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape6)/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 7: Adding Expert Knowledge (with size 4) to input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape7=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "    \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(2*n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val]\n",
    "        val_p = series_pred[-(2*n_val+window_size):]\n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred[l][-6:]\n",
    "            for l in range(val_pred.shape[0]):\n",
    "                val_pred[l]=val_pred[l][-6:]\n",
    "            for l in range(test_pred.shape[0]):\n",
    "                test_pred[l]=test_pred[l][-6:]\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        #train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        #val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        #test_pred_auto=pd.DataFrame(test_pred_auto)\n",
    "\n",
    "        #train_pred_final=np.array(encoder2.predict(train_pred_norm_auto))\n",
    "        #val_pred_final=np.array(encoder2.predict(val_pred_norm_auto))\n",
    "        #test_pred_final=np.array(encoder2.predict(test_pred_auto))\n",
    "\n",
    "        train_pred_final=train_pred_norm_auto\n",
    "        val_pred_final=val_pred_norm_auto\n",
    "        test_pred_final=test_pred_auto\n",
    "        ############################################################################################################\n",
    "        train_input = np.append(x_train,train_pred_final.reshape(train_pred_final.shape[0],train_pred_final.shape[1],1),axis=1)\n",
    "        val_input = np.append(x_val,val_pred_final.reshape(val_pred_final.shape[0],val_pred_final.shape[1],1),axis=1)\n",
    "        test_input = np.append(x_test,test_pred_final.reshape(test_pred_final.shape[0],test_pred_final.shape[1],1),axis=1)\n",
    "        \n",
    "        \n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size+4,1),name='input_data')\n",
    "        #input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF07.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF07.h5')\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape7[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction07.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.209504387588556\n"
     ]
    }
   ],
   "source": [
    "print (sum(Final_smape7)/72)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
