{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects.numpy2ri\n",
    "from stldecompose import decompose\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "stats = importr('stats')\n",
    "stl=stats.stl\n",
    "ts =stats.ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 123)\n",
      "(72, 90)\n"
     ]
    }
   ],
   "source": [
    "data= pd.read_csv(\"Data/FromMohammed/cif_dataset_complete.csv\",header=None)\n",
    "predictions = pd.read_csv(\"Data/FromMohammed/theta_25_horg.csv\",index_col=0,skiprows = [1])\n",
    "print(data.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "#     depth=data.shape[2]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size+1-horizon):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length-window_size+1-horizon,horizon])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "#     print(str(extra))\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "#     print(data)\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],extra)\n",
    "    data = np.append(data,data_app)    \n",
    "#     data = np.append(data,np.zeros([horizon-extra]))\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smape(val,preds,horizon):\n",
    "    temp = np.abs(val-preds)\n",
    "    temp1=np.abs(val)+np.abs(preds)\n",
    "    \n",
    "    smape = 200/horizon*np.sum(temp/temp1)\n",
    "\n",
    "    return smape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def smape_nzr(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200.0 * np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case1: Using only Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 17:41:36.165477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [24:33<00:00, 23.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 18:06:10.094391\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "14.968376465078194\n",
      "SMAPE2 (NZR approach)\n",
      "14.968376465078194\n",
      "MSE1:\n",
      "5347278798391.24\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape1=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "####################################################################################\n",
    "##############creating train, validation and test sets from Data####################\n",
    "####################################################################################\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_d=current_row\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        #val = series_data[-(2*n_val+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "        \n",
    "        \n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "################    Creating inputs for Neural Network      ########################\n",
    "####################################################################################\n",
    "        #train_input = np.append(x_train,train_pred,axis=1)\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred,axis=1)\n",
    "        test_input = x_test\n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        #input_data= Input(batch_shape=(None,window_size+horizon,1),name='input_data')\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "\n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF01.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "        model.load_weights('MyModelCheckpoint/CIF01.h5')\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape1[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "    \n",
    "np.savetxt('MyModelCheckpoint/prediction01.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Adding Expert Knowledge to layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape2=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val+3]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        val_input = x_val\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val+3]\n",
    "        #val_p = series_pred[-(2*n_val+window_size):]\n",
    "        val_p = series_pred[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "##############################################################################################################################\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l][0]=(train_pred1[l][0]+train_pred1[l][6])/2\n",
    "                train_pred[l][1]=(train_pred1[l][1]+train_pred1[l][7])/2\n",
    "                train_pred[l][2]=(train_pred1[l][2]+train_pred1[l][8])/2\n",
    "                train_pred[l][3]=(train_pred1[l][3]+train_pred1[l][9])/2\n",
    "                train_pred[l][4]=(train_pred1[l][4]+train_pred1[l][10])/2\n",
    "                train_pred[l][5]=(train_pred1[l][5]+train_pred1[l][11])/2\n",
    "            for l in range(val_pred1.shape[0]):\n",
    "                val_pred[l][0]=(val_pred1[l][0]+val_pred1[l][6])/2\n",
    "                val_pred[l][1]=(val_pred1[l][1]+val_pred1[l][7])/2\n",
    "                val_pred[l][2]=(val_pred1[l][2]+val_pred1[l][8])/2\n",
    "                val_pred[l][3]=(val_pred1[l][3]+val_pred1[l][9])/2\n",
    "                val_pred[l][4]=(val_pred1[l][4]+val_pred1[l][10])/2\n",
    "                val_pred[l][5]=(val_pred1[l][5]+val_pred1[l][11])/2\n",
    "            for l in range(test_pred1.shape[0]):\n",
    "                test_pred[l][0]=(test_pred1[l][0]+test_pred1[l][6])/2\n",
    "                test_pred[l][1]=(test_pred1[l][1]+test_pred1[l][7])/2\n",
    "                test_pred[l][2]=(test_pred1[l][2]+test_pred1[l][8])/2\n",
    "                test_pred[l][3]=(test_pred1[l][3]+test_pred1[l][9])/2\n",
    "                test_pred[l][4]=(test_pred1[l][4]+test_pred1[l][10])/2\n",
    "                test_pred[l][5]=(test_pred1[l][5]+test_pred1[l][11])/2\n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                    \n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,4),name='input_pred')\n",
    "        \n",
    "        encoded12 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "        encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "        \n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=add([branch_0,encoded32])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        #branch_1=add([branch_1,encoded32])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        #branch_2=add([branch_2,encoded32])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "        \n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF02.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF02.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape2[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction02.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape2)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 16:56:45.182553\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "15.122601775492958\n",
      "SMAPE2 (NZR approach)\n",
      "15.122601775492955\n",
      "MSE1:\n",
      "5647516721968.171\n",
      "15.122601775492958\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape2)/72)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Adding Expert Knowledge to layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape3=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val+3]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val+3]\n",
    "        #val_p = series_pred[-(2*n_val+window_size):]\n",
    "        val_p = series_pred[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "##############################################################################################################################\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred1[l][-6:]\n",
    "            for l in range(val_pred1.shape[0]):\n",
    "                val_pred[l]=val_pred1[l][-6:]\n",
    "            for l in range(test_pred1.shape[0]):\n",
    "                test_pred[l]=test_pred1[l][-6:]            \n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                    \n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,4),name='input_pred')\n",
    "        \n",
    "        encoded12 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "        encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "        \n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        #branch_0=add([branch_0,encoded32])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_1=add([branch_1,encoded32])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        #branch_2=add([branch_2,encoded32])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "        \n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF03.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF03.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape3[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction03.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape3)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 17:41:35.556115\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "15.11659116681655\n",
      "SMAPE2 (NZR approach)\n",
      "15.11659116681655\n",
      "MSE1:\n",
      "5740811068349.316\n",
      "15.11659116681655\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape3)/72)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4: Adding Expert Knowledge to layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape4=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val+3]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        val_input = x_val\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val+3]\n",
    "        val_p = series_pred[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "##############################################################################################################################\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred1[l][-6:]\n",
    "            for l in range(val_pred1.shape[0]):\n",
    "                val_pred[l]=val_pred1[l][-6:]\n",
    "            for l in range(test_pred1.shape[0]):\n",
    "                test_pred[l]=test_pred1[l][-6:]\n",
    "            \n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                    \n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,4),name='input_pred')\n",
    "        \n",
    "        encoded12 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "        encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "        \n",
    "\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        #branch_0=add([branch_0,encoded32])\n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        #branch_1=add([branch_1,encoded32])\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_2=add([branch_2,encoded32])\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "        \n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF04.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF04.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape4[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction04.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape4)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 19:17:18.337923\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "15.668842914630002\n",
      "SMAPE2 (NZR approach)\n",
      "15.668842914630002\n",
      "MSE1:\n",
      "5916131090432.906\n",
      "15.668842914630002\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape4)/72)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5: Adding Expert Knowledge to All 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape5=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "    #for y in range(1):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val+3]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        #val = series_data[-(2*n_val+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val+3]\n",
    "        #val_p = series_pred[-(2*n_val+window_size):]\n",
    "        val_p = series_pred[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "##############################################################################################################################\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred1[l][-6:]\n",
    "            for l in range(val_pred1.shape[0]):\n",
    "                val_pred[l]=val_pred1[l][-6:]\n",
    "            for l in range(test_pred1.shape[0]):\n",
    "                test_pred[l]=test_pred1[l][-6:]            \n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                    \n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,4),name='input_pred')\n",
    "        \n",
    "        encoded11 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded21 = Dense(16, activation = 'relu')(encoded11)\n",
    "        encoded31 = Dense(32, activation = 'relu')(encoded21)\n",
    "        \n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=add([branch_0,encoded31])\n",
    "        \n",
    "        encoded12 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "        encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "        \n",
    "        branch_1 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_1=add([branch_1,encoded32])\n",
    "        \n",
    "        \n",
    "        encoded13 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded23 = Dense(16, activation = 'relu')(encoded13)\n",
    "        encoded33 = Dense(32, activation = 'relu')(encoded23)\n",
    "        branch_2 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_2=add([branch_2,encoded33])\n",
    "        \n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "        \n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF05.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF05.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape5[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction05.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape5)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-13 20:12:33.812147\n",
      "-----------------\n",
      "SMAPE1: (DeepEX approach)\n",
      "15.527161869177696\n",
      "SMAPE2 (NZR approach)\n",
      "15.527161869177696\n",
      "MSE1:\n",
      "5599537400019.062\n",
      "15.527161869177696\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape5)/72)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Case 6: Using conv64 + Adding Expert Knowledge to All 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-14 13:34:08.593499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/72 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  1%|█▏                                                                                 | 1/72 [00:05<06:04,  5.14s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  3%|██▎                                                                                | 2/72 [00:10<05:57,  5.11s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  4%|███▍                                                                               | 3/72 [00:15<06:02,  5.25s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  6%|████▌                                                                              | 4/72 [00:22<06:27,  5.70s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  7%|█████▊                                                                             | 5/72 [00:29<06:54,  6.19s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "  8%|██████▉                                                                            | 6/72 [00:37<07:13,  6.57s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 10%|████████                                                                           | 7/72 [00:45<07:46,  7.17s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 11%|█████████▏                                                                         | 8/72 [00:55<08:17,  7.78s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 12%|██████████▍                                                                        | 9/72 [01:04<08:36,  8.20s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 14%|███████████▍                                                                      | 10/72 [01:15<09:16,  8.98s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 15%|████████████▌                                                                     | 11/72 [01:27<10:07,  9.97s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 17%|█████████████▋                                                                    | 12/72 [01:38<10:27, 10.46s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 18%|██████████████▊                                                                   | 13/72 [01:51<10:58, 11.16s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 19%|███████████████▉                                                                  | 14/72 [02:06<11:46, 12.18s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 21%|█████████████████                                                                 | 15/72 [02:22<12:40, 13.35s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 22%|██████████████████▏                                                               | 16/72 [02:37<13:02, 13.98s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 24%|███████████████████▎                                                              | 17/72 [02:54<13:31, 14.75s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 25%|████████████████████▌                                                             | 18/72 [03:11<13:53, 15.43s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 26%|█████████████████████▋                                                            | 19/72 [03:28<14:09, 16.03s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 28%|██████████████████████▊                                                           | 20/72 [03:47<14:29, 16.73s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 29%|███████████████████████▉                                                          | 21/72 [04:06<14:48, 17.41s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 31%|█████████████████████████                                                         | 22/72 [04:26<15:12, 18.26s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▏                                                       | 23/72 [04:48<15:45, 19.30s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 33%|███████████████████████████▎                                                      | 24/72 [05:08<15:48, 19.76s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 35%|████████████████████████████▍                                                     | 25/72 [05:30<15:52, 20.27s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 36%|█████████████████████████████▌                                                    | 26/72 [05:52<15:58, 20.83s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 38%|██████████████████████████████▊                                                   | 27/72 [06:15<16:09, 21.54s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 39%|███████████████████████████████▉                                                  | 28/72 [06:39<16:12, 22.09s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 40%|█████████████████████████████████                                                 | 29/72 [07:02<16:03, 22.40s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 42%|██████████████████████████████████▏                                               | 30/72 [07:28<16:33, 23.67s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 43%|███████████████████████████████████▎                                              | 31/72 [07:55<16:49, 24.63s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 44%|████████████████████████████████████▍                                             | 32/72 [08:23<17:07, 25.69s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 46%|█████████████████████████████████████▌                                            | 33/72 [08:51<17:03, 26.25s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 47%|██████████████████████████████████████▋                                           | 34/72 [09:19<17:00, 26.86s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 49%|███████████████████████████████████████▊                                          | 35/72 [09:48<16:57, 27.51s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 50%|█████████████████████████████████████████                                         | 36/72 [10:17<16:46, 27.97s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 51%|██████████████████████████████████████████▏                                       | 37/72 [10:49<16:55, 29.01s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 53%|███████████████████████████████████████████▎                                      | 38/72 [11:20<16:51, 29.76s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 54%|████████████████████████████████████████████▍                                     | 39/72 [11:55<17:09, 31.20s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 56%|█████████████████████████████████████████████▌                                    | 40/72 [12:32<17:32, 32.91s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 57%|██████████████████████████████████████████████▋                                   | 41/72 [13:08<17:32, 33.96s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 58%|███████████████████████████████████████████████▊                                  | 42/72 [13:44<17:16, 34.54s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 60%|████████████████████████████████████████████████▉                                 | 43/72 [14:23<17:19, 35.85s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 61%|██████████████████████████████████████████████████                                | 44/72 [15:00<16:52, 36.16s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 62%|███████████████████████████████████████████████████▎                              | 45/72 [15:38<16:36, 36.89s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▍                             | 46/72 [16:20<16:36, 38.31s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 65%|█████████████████████████████████████████████████████▌                            | 47/72 [17:00<16:06, 38.67s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 67%|██████████████████████████████████████████████████████▋                           | 48/72 [17:40<15:39, 39.13s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 68%|███████████████████████████████████████████████████████▊                          | 49/72 [18:20<15:08, 39.52s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 69%|████████████████████████████████████████████████████████▉                         | 50/72 [19:00<14:31, 39.63s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 71%|██████████████████████████████████████████████████████████                        | 51/72 [19:41<14:01, 40.07s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 72%|███████████████████████████████████████████████████████████▏                      | 52/72 [20:22<13:26, 40.35s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 74%|████████████████████████████████████████████████████████████▎                     | 53/72 [21:04<12:55, 40.81s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 54/72 [21:46<12:21, 41.21s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 76%|██████████████████████████████████████████████████████████████▋                   | 55/72 [22:26<11:35, 40.89s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 78%|███████████████████████████████████████████████████████████████▊                  | 56/72 [23:09<11:03, 41.44s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 79%|████████████████████████████████████████████████████████████████▉                 | 57/72 [23:52<10:29, 41.99s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 81%|██████████████████████████████████████████████████████████████████                | 58/72 [24:35<09:52, 42.31s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 82%|███████████████████████████████████████████████████████████████████▏              | 59/72 [25:20<09:19, 43.02s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 60/72 [26:08<08:54, 44.58s/it]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 61/72 [26:55<08:16, 45.15s/it]"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "smape1=0\n",
    "smape2=0\n",
    "mse1=0\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,12])\n",
    "    final_test = np.zeros([data_length,12])\n",
    "    Final_smape6=np.zeros(data_length)\n",
    "    for y in range(data_length):\n",
    "    #for y in range(1):\n",
    "        final_predictions = np.zeros([data_length,12])\n",
    "        final_test = np.zeros([data_length,12])\n",
    "        horizon = data.iloc[y].values[1]\n",
    "        window_size=6\n",
    "        current_row =np.asarray(data.loc[y][3:].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] # values after (rr)th position in current_row\n",
    "#####################################################################\n",
    "        series_data = series_d[:-horizon]\n",
    "        series_length = series_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        if n_val < horizon: #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            n_val = horizon#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "        #train = series_data[:-n_val]\n",
    "        train = series_data[:-n_val+3]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        test = series_d[-(horizon+window_size):]\n",
    "        #val = series_data[-(2*n_val+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler1 = MinMaxScaler()\n",
    "            scaler1.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler1.transform(train_sequence[0][j])\n",
    "            #scaler1.fit(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1))\n",
    "            train_sequence_norm[1][j]= scaler1.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "    \n",
    "\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler2 = MinMaxScaler()\n",
    "            scaler2.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler2.transform(val_sequence[0][j])\n",
    "            #scaler2.fit(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1))\n",
    "            val_sequence_norm[1][j]= scaler2.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler3 = MinMaxScaler()\n",
    "            scaler3.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler3.transform(test_sequence[0][j])\n",
    "            #scaler3.fit(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1))\n",
    "            test_sequence_norm[1][j]= scaler3.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "    \n",
    "        current_test=test_sequence[1][0]\n",
    "\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]\n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "############################################################################################################\n",
    "        train_input = x_train\n",
    "        #val_input = np.append(x_val,val_pred_auto,axis=1)\n",
    "        val_input = x_val\n",
    "        #test_input = np.append(x_test,test_pred_auto,axis=1)\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "#####################################################################\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-horizon]\n",
    "        train_p = series_pred[:-n_val+3]\n",
    "        #val_p = series_pred[-(2*n_val+window_size):]\n",
    "        val_p = series_pred[-(n_val+window_size):]#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        test_p = series_p[-(horizon+window_size):]\n",
    "\n",
    "        train_pred1 = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred1 = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred1 = nonov_make_k_input(test_p,window_size,horizon)\n",
    "\n",
    "##############################################################################################################################\n",
    "        if horizon==12 :\n",
    "            train_pred = np.zeros((train_pred1.shape[0], 6, train_pred1.shape[2]))\n",
    "            val_pred = np.zeros((val_pred1.shape[0], 6, val_pred1.shape[2]))\n",
    "            test_pred = np.zeros((test_pred1.shape[0], 6, test_pred1.shape[2]))\n",
    "\n",
    "            for l in range(train_pred1.shape[0]):\n",
    "                train_pred[l]=train_pred1[l][-6:]\n",
    "            for l in range(val_pred1.shape[0]):\n",
    "                val_pred[l]=val_pred1[l][-6:]\n",
    "            for l in range(test_pred1.shape[0]):\n",
    "                test_pred[l]=test_pred1[l][-6:]\n",
    "            \n",
    "        else:\n",
    "            train_pred=train_pred1\n",
    "            val_pred=val_pred1\n",
    "            test_pred=test_pred1\n",
    "    \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler.transform(test_pred[j])\n",
    "    \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],6)\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],6)\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],6)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "        ncol = 6\n",
    "        encoding_dim = 4\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        #encoder = Model(inputs = input_dim, outputs = encoded2)\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('MyModelCheckpoint/encoder_model.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))\n",
    "\n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                    \n",
    "\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,4),name='input_pred')\n",
    "        \n",
    "        encoded11 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded21 = Dense(16, activation = 'relu')(encoded11)\n",
    "        encoded31 = Dense(32, activation = 'relu')(encoded21)\n",
    "        encoded31 = Dense(64, activation = 'relu')(encoded31)\n",
    "        \n",
    "        branch_0 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=add([branch_0,encoded31])\n",
    "        \n",
    "        encoded12 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded22 = Dense(16, activation = 'relu')(encoded12)\n",
    "        encoded32 = Dense(32, activation = 'relu')(encoded22)\n",
    "        encoded32 = Dense(64, activation = 'relu')(encoded32)\n",
    "        \n",
    "        branch_1 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_1=add([branch_1,encoded32])\n",
    "        \n",
    "        \n",
    "        encoded13 = Dense(8, activation = 'relu')(input_pred)\n",
    "        encoded23 = Dense(16, activation = 'relu')(encoded13)\n",
    "        encoded33 = Dense(32, activation = 'relu')(encoded23)\n",
    "        encoded33 = Dense(64, activation = 'relu')(encoded33)\n",
    "        \n",
    "        branch_2 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_2=add([branch_2,encoded33])\n",
    "        \n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_3)\n",
    "        \n",
    "\n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='MyModelCheckpoint/CIF07.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "\n",
    "\n",
    "        model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights('MyModelCheckpoint/CIF07.h5')\n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler3.inverse_transform(pred)#######################################################\n",
    "\n",
    "        final_predictions[y,:horizon] = pred.reshape(horizon)\n",
    "        final_test[y,:horizon]=test[-horizon:]\n",
    "\n",
    "        AAA=smape(pred.reshape(horizon),current_test,horizon)\n",
    "        smape1+=AAA\n",
    "        smape2+=smape_nzr(pred.reshape(horizon),current_test)\n",
    "        Final_smape6[y]=AAA\n",
    "        mse1+=mse(pred.reshape(horizon),current_test)\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('MyModelCheckpoint/prediction07.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape6)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())\n",
    "print('-----------------')\n",
    "print(\"SMAPE1: (DeepEX approach)\")\n",
    "print(smape1/72)\n",
    "print(\"SMAPE2 (NZR approach)\")\n",
    "print(smape2/72)\n",
    "print(\"MSE1:\")\n",
    "print(mse1/72)\n",
    "print (sum(Final_smape6)/72)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
