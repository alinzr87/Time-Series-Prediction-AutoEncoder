{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 791)\n",
      "(111, 594)\n"
     ]
    }
   ],
   "source": [
    "# Main Dataset\n",
    "data= pd.read_csv(\"Data/NN5_interpolated.csv\",header=None)\n",
    "# Expert Knowledge\n",
    "predictions = pd.read_csv(\"Data/NN5_theta_25_horg.csv\",index_col=0,skiprows = [1])\n",
    "print(data.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Make Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "#     depth=data.shape[2]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size+1-horizon):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length-window_size+1-horizon,horizon])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "#     print(str(extra))\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "#     print(data)\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "def nonov_make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],extra)\n",
    "    data = np.append(data,data_app)    \n",
    "#     data = np.append(data,np.zeros([horizon-extra]))\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function (SMAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200.0 * np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder for Expert Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 7s - loss: 0.0264\n",
      "Epoch 2/500\n",
      " - 5s - loss: 0.0121\n",
      "Epoch 3/500\n",
      " - 6s - loss: 0.0082\n",
      "Epoch 4/500\n",
      " - 6s - loss: 0.0057\n",
      "Epoch 5/500\n",
      " - 7s - loss: 0.0051\n",
      "Epoch 6/500\n",
      " - 6s - loss: 0.0047\n",
      "Epoch 7/500\n",
      " - 7s - loss: 0.0042\n",
      "Epoch 8/500\n",
      " - 7s - loss: 0.0040\n",
      "Epoch 9/500\n",
      " - 6s - loss: 0.0039\n",
      "Epoch 10/500\n",
      " - 5s - loss: 0.0038\n",
      "Epoch 11/500\n",
      " - 6s - loss: 0.0036\n",
      "Epoch 12/500\n",
      " - 5s - loss: 0.0035\n",
      "Epoch 13/500\n",
      " - 5s - loss: 0.0034\n",
      "Epoch 14/500\n",
      " - 5s - loss: 0.0034\n",
      "Epoch 15/500\n",
      " - 5s - loss: 0.0033\n",
      "Epoch 16/500\n",
      " - 5s - loss: 0.0032\n",
      "Epoch 17/500\n",
      " - 5s - loss: 0.0030\n",
      "Epoch 18/500\n",
      " - 5s - loss: 0.0029\n",
      "Epoch 19/500\n",
      " - 5s - loss: 0.0028\n",
      "Epoch 20/500\n",
      " - 5s - loss: 0.0027\n",
      "Epoch 21/500\n",
      " - 5s - loss: 0.0026\n",
      "Epoch 22/500\n",
      " - 5s - loss: 0.0026\n",
      "Epoch 23/500\n",
      " - 5s - loss: 0.0026\n",
      "Epoch 24/500\n",
      " - 5s - loss: 0.0025\n",
      "Epoch 25/500\n",
      " - 5s - loss: 0.0025\n",
      "Epoch 26/500\n",
      " - 5s - loss: 0.0024\n",
      "Epoch 27/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 28/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 29/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 30/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 31/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 32/500\n",
      " - 5s - loss: 0.0023\n",
      "Epoch 33/500\n",
      " - 6s - loss: 0.0022\n",
      "Epoch 34/500\n",
      " - 5s - loss: 0.0022\n",
      "Epoch 35/500\n",
      " - 5s - loss: 0.0022\n",
      "Epoch 36/500\n",
      " - 5s - loss: 0.0022\n",
      "Epoch 37/500\n",
      " - 5s - loss: 0.0022\n",
      "Epoch 38/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 39/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 40/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 41/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 42/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 43/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 44/500\n",
      " - 5s - loss: 0.0021\n",
      "Epoch 45/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 46/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 47/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 48/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 49/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 50/500\n",
      " - 5s - loss: 0.0020\n",
      "Epoch 51/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 52/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 53/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 54/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 55/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 56/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 57/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 58/500\n",
      " - 5s - loss: 0.0019\n",
      "Epoch 59/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 60/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 61/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 62/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 63/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 64/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 65/500\n",
      " - 5s - loss: 0.0018\n",
      "Epoch 66/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 67/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 68/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 69/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 70/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 71/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 72/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 73/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 74/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 75/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 76/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 77/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 78/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 79/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 80/500\n",
      " - 5s - loss: 0.0017\n",
      "Epoch 81/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 82/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 83/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 84/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 85/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 86/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 87/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 88/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 89/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 90/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 91/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 92/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 93/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 94/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 95/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 96/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 97/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 98/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 99/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 100/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 101/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 102/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 103/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 104/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 105/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 106/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 107/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 108/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 109/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 110/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 111/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 112/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 113/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 114/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 115/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 116/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 117/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 118/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 119/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 120/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 121/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 122/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 123/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 124/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 125/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 126/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 127/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 128/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 129/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 130/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 131/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 132/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 133/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 134/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 135/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 136/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 137/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 138/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 139/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 140/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 141/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 142/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 143/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 144/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 145/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 146/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 147/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 148/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 149/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 150/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 151/500\n",
      " - 8s - loss: 0.0016\n",
      "Epoch 152/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 153/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 154/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 155/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 156/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 157/500\n",
      " - 7s - loss: 0.0016\n",
      "Epoch 158/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 159/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 160/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 161/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 162/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 163/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 164/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 165/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 166/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 167/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 168/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 169/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 170/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 171/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 172/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 173/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 174/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 175/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 176/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 177/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 178/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 179/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 180/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 181/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 182/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 183/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 184/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 185/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 186/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 187/500\n",
      " - 6s - loss: 0.0016\n",
      "Epoch 188/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 189/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 190/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 191/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 192/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 193/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 194/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 195/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 196/500\n",
      " - 5s - loss: 0.0016\n",
      "Epoch 197/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 198/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 199/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 200/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 201/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 202/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 203/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 204/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 205/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 206/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 207/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 208/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 209/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 210/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 211/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 212/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 213/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 214/500\n",
      " - 6s - loss: 0.0015\n",
      "Epoch 215/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 216/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 217/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 218/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 219/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 220/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 221/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 222/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 223/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 224/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 225/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 226/500\n",
      " - 6s - loss: 0.0015\n",
      "Epoch 227/500\n",
      " - 7s - loss: 0.0015\n",
      "Epoch 228/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 229/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 230/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 231/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 232/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 233/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 234/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 235/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 236/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 237/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 238/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 239/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 240/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 241/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 242/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 243/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 244/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 245/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 246/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 247/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 248/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 249/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 250/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 251/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 252/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 253/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 254/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 255/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 256/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 257/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 258/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 259/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 260/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 261/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 262/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 263/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 264/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 265/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 266/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 267/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 268/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 269/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 270/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 271/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 272/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 273/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 274/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 275/500\n",
      " - 5s - loss: 0.0015\n",
      "Epoch 276/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 277/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 278/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 279/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 280/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 281/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 282/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 283/500\n",
      " - 7s - loss: 0.0014\n",
      "Epoch 284/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 285/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 286/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 287/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 288/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 289/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 290/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 291/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 292/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 293/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 294/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 295/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 296/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 297/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 298/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 299/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 300/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 301/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 302/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 303/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 304/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 305/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 306/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 307/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 308/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 309/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 310/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 311/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 312/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 313/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 314/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 315/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 316/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 317/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 318/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 319/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 320/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 321/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 322/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 323/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 324/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 325/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 326/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 327/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 328/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 329/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 330/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 331/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 332/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 333/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 334/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 335/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 336/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 337/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 338/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 339/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 340/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 341/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 342/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 343/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 344/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 345/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 346/500\n",
      " - 8s - loss: 0.0014\n",
      "Epoch 347/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 348/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 349/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 350/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 351/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 352/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 353/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 354/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 355/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 356/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 357/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 358/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 359/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 360/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 361/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 362/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 363/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 364/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 365/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 366/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 367/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 368/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 369/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 370/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 371/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 372/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 373/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 374/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 375/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 376/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 377/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 378/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 379/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 380/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 381/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 382/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 383/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 384/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 385/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 386/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 387/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 388/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 389/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 390/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 391/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 392/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 393/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 394/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 395/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 396/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 397/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 398/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 399/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 400/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 401/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 402/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 403/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 404/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 405/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 406/500\n",
      " - 7s - loss: 0.0014\n",
      "Epoch 407/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 408/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 409/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 410/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 411/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 412/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 413/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 414/500\n",
      " - 5s - loss: 0.0014\n",
      "Epoch 415/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 416/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 417/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 418/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 419/500\n",
      " - 6s - loss: 0.0014\n",
      "Epoch 420/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 421/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 422/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 423/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 424/500\n",
      " - 7s - loss: 0.0013\n",
      "Epoch 425/500\n",
      " - 7s - loss: 0.0013\n",
      "Epoch 426/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 427/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 428/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 429/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 430/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 431/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 432/500\n",
      " - 7s - loss: 0.0013\n",
      "Epoch 433/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 434/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 435/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 436/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 437/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 438/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 439/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 440/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 441/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 442/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 443/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 444/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 445/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 446/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 447/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 448/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 449/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 450/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 451/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 452/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 453/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 454/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 455/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 456/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 457/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 458/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 459/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 460/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 461/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 462/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 463/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 464/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 465/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 466/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 467/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 468/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 469/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 470/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 471/500\n",
      " - 8s - loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/500\n",
      " - 7s - loss: 0.0013\n",
      "Epoch 473/500\n",
      " - 9s - loss: 0.0013\n",
      "Epoch 474/500\n",
      " - 7s - loss: 0.0013\n",
      "Epoch 475/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 476/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 477/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 478/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 479/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 480/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 481/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 482/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 483/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 484/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 485/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 486/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 487/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 488/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 489/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 490/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 491/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 492/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 493/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 494/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 495/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 496/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 497/500\n",
      " - 6s - loss: 0.0013\n",
      "Epoch 498/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 499/500\n",
      " - 5s - loss: 0.0013\n",
      "Epoch 500/500\n",
      " - 5s - loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "theta_length = predictions.shape[0]\n",
    "horizon=56\n",
    "window_size=70\n",
    "theta_all=np.zeros(0)\n",
    "for i in range(theta_length):\n",
    "    current_pred= np.asarray(predictions.iloc[i].dropna().values,dtype=float)\n",
    "    theta_all=np.concatenate((theta_all,current_pred),axis=0)\n",
    "theta_input = make_k_input(theta_all,window_size,horizon)\n",
    "normalized_theta_input=np.zeros(theta_input.shape)\n",
    "for j in range(theta_input.shape[0]):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(theta_input[j])\n",
    "    normalized_theta_input[j]= scaler.transform(theta_input[j])\n",
    "#######################################################################################\n",
    "norm_reshaped_input=normalized_theta_input.reshape(normalized_theta_input.shape[0],56)\n",
    "ncol = normalized_theta_input.shape[1] #56\n",
    "\n",
    "encoding_dim = 32\n",
    "input_dim = Input(shape = (ncol, ))\n",
    "# Encoder Layers\n",
    "encoded1 = Dense(48, activation = 'relu')(input_dim)\n",
    "encoded2 = Dense(40, activation = 'relu')(encoded1)\n",
    "encoded3 = Dense(encoding_dim, activation = 'relu')(encoded2)\n",
    "# Decoder Layers\n",
    "decoded1 = Dense(40, activation = 'relu')(encoded3)\n",
    "decoded2 = Dense(48, activation = 'sigmoid')(decoded1)\n",
    "decoded3 = Dense(ncol, activation = 'sigmoid')(decoded2)\n",
    "# Combine Encoder and Deocder layers\n",
    "autoencoder = Model(inputs = input_dim, outputs = decoded3)\n",
    "autoencoder.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "autoencoder.fit(norm_reshaped_input, norm_reshaped_input, epochs = 500, batch_size = 32, shuffle = False,verbose=2)\n",
    "encoder = Model(inputs = input_dim, outputs = encoded3)\n",
    "encoded_input = Input(shape = (encoding_dim, ))\n",
    "auto_out = np.array(encoder.predict(norm_reshaped_input))\n",
    "\n",
    "new_theta_input=auto_out.reshape(auto_out.shape[0],auto_out.shape[1],1)\n",
    "autoencoder.save('Output/autoencoder_model_NN5_w70.h5')\n",
    "encoder.save('Output/encoder_model_NN5_w70.h5')\n",
    "np.savetxt('Output/new_theta_input_NN5_w70.csv',auto_out, fmt='%1.3f',delimiter=',')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1 : Only Data (75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 111/111 [2:13:20<00:00, 74.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_data (InputLayer)      (None, 70, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 70, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 70, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 70, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 56)                250936    \n",
      "=================================================================\n",
      "Total params: 269,624\n",
      "Trainable params: 269,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-----------------\n",
      "SMAPE:\n",
      "23.098337473831062\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,56])\n",
    "    final_test = np.zeros([data_length,56])\n",
    "    final_smape=np.zeros(data_length)\n",
    "    for y in range(data_length):                \n",
    "        num_test=56\n",
    "        horizon=56\n",
    "        window_size=70\n",
    "        current_row =np.asarray(data.loc[y].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] \n",
    "        series_data = series_d[:-num_test]\n",
    "        series_length = series_data.size\n",
    "        current_test=series_d[-num_test:]\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(num_test+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler.transform(train_sequence[0][j])\n",
    "            train_sequence_norm[1][j]= scaler.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler.transform(val_sequence[0][j])\n",
    "            val_sequence_norm[1][j]= scaler.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler.transform(test_sequence[0][j])\n",
    "            test_sequence_norm[1][j]= scaler.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "            \n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]        \n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]             \n",
    "\n",
    "        train_input = x_train\n",
    "        val_input = x_val\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################        \n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        K.clear_session()\n",
    "        \n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        \n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.00001))(branch_3)\n",
    "        \n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='NetworkWeights_Checkpoints/checkpoint_NN5_01_%i.h5' %y,monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001))\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=12,shuffle=True, epochs=200,verbose=0)\n",
    "        \n",
    "        model.load_weights('NetworkWeights_Checkpoints/checkpoint_NN5_01_%i.h5' %y)\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler.inverse_transform(pred)\n",
    "\n",
    "        final_predictions[y,:num_test] = pred.reshape(num_test)\n",
    "        final_test[y,:num_test]=test[-num_test:]\n",
    "\n",
    "        AAA=smape(pred.reshape(num_test),current_test)       \n",
    "        final_smape[y]=AAA\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('Output/prediction_NN5_01.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "model.summary()\n",
    "print('-----------------')\n",
    "print(\"SMAPE:\")\n",
    "print (sum(final_smape)/data_length)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only data (100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 111/111 [5:40:12<00:00, 212.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_data (InputLayer)      (None, 70, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 70, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 70, 128)           49280     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 70, 128)           49280     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8960)              0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 56)                501816    \n",
      "=================================================================\n",
      "Total params: 600,888\n",
      "Trainable params: 600,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-----------------\n",
      "SMAPE:\n",
      "22.936982182913102\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,56])\n",
    "    final_test = np.zeros([data_length,56])\n",
    "    final_smape=np.zeros(data_length)\n",
    "    for y in range(data_length):                \n",
    "        num_test=56\n",
    "        horizon=56\n",
    "        window_size=70\n",
    "        current_row =np.asarray(data.loc[y].dropna().values,dtype=float)\n",
    "        #rr = current_row.size\n",
    "        #rr = int(np.floor(rr*.25))\n",
    "        #series_d=current_row[rr:] \n",
    "        series_d=current_row\n",
    "        series_data = series_d[:-num_test]\n",
    "        series_length = series_data.size\n",
    "        current_test=series_d[-num_test:]\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(num_test+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler.transform(train_sequence[0][j])\n",
    "            train_sequence_norm[1][j]= scaler.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler.transform(val_sequence[0][j])\n",
    "            val_sequence_norm[1][j]= scaler.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler.transform(test_sequence[0][j])\n",
    "            test_sequence_norm[1][j]= scaler.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "            \n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]        \n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]             \n",
    "\n",
    "        train_input = x_train\n",
    "        val_input = x_val\n",
    "        test_input = x_test\n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################        \n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        K.clear_session()\n",
    "        \n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        \n",
    "        branch_0 = Conv1D(128,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_1 = Conv1D(128,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(128,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.00001))(branch_3)\n",
    "        \n",
    "        model=Model(inputs=[input_data],outputs=net)\n",
    "        callback = ModelCheckpoint(filepath='NetworkWeights_Checkpoints/checkpoint_NN5_011_%i.h5' %y,monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001))\n",
    "        model.fit({'input_data':train_input},y_train,validation_data=[[val_input],y_val],callbacks=[callback],batch_size=12,shuffle=True, epochs=200,verbose=0)\n",
    "        \n",
    "        model.load_weights('NetworkWeights_Checkpoints/checkpoint_NN5_011_%i.h5' %y)\n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler.inverse_transform(pred)\n",
    "\n",
    "        final_predictions[y,:num_test] = pred.reshape(num_test)\n",
    "        final_test[y,:num_test]=test[-num_test:]\n",
    "\n",
    "        AAA=smape(pred.reshape(num_test),current_test)       \n",
    "        final_smape[y]=AAA\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('Output/prediction_NN5_011.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "model.summary()\n",
    "print('-----------------')\n",
    "print(\"SMAPE:\")\n",
    "print (sum(final_smape)/data_length)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Data + Expert Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [01:18<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 70, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_pred (InputLayer)         (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 70, 32)       128         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 32)        0           input_pred[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 71, 32)       0           conv1d_1[0][0]                   \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 71, 64)       6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 71, 64)       12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4544)         0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_final (Dense)             (None, 56)           254520      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 273,208\n",
      "Trainable params: 273,208\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-----------------\n",
      "SMAPE:\n",
      "22.220027102772633\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "data_length = data.shape[0]\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,56])\n",
    "    final_test = np.zeros([data_length,56])\n",
    "    final_smape=np.zeros(data_length)    \n",
    "    for y in range(data_length):                    \n",
    "        num_test=56\n",
    "        horizon=56\n",
    "        window_size=70\n",
    "        current_row =np.asarray(data.loc[y].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] \n",
    "        series_data = series_d[:-num_test]\n",
    "        series_length = series_data.size\n",
    "        current_test=series_d[-num_test:]\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        \n",
    "        train = series_data[:-n_val]\n",
    "        test = series_d[-(num_test+window_size):]\n",
    "        val = series_data[-(n_val+window_size):]\n",
    "\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        train_sequence_norm = deepcopy(train_sequence)\n",
    "        val_sequence_norm = deepcopy(val_sequence)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        for j in range(train_sequence[0].shape[0]):\n",
    "            scaler.fit(train_sequence[0][j])\n",
    "            train_sequence_norm[0][j]= scaler.transform(train_sequence[0][j])\n",
    "            train_sequence_norm[1][j]= scaler.transform(train_sequence[1][j].reshape(train_sequence[1][j].shape[0],1)).reshape(train_sequence[1][j].shape[0])\n",
    "        for j in range(val_sequence[0].shape[0]):\n",
    "            scaler.fit(val_sequence[0][j])\n",
    "            val_sequence_norm[0][j]= scaler.transform(val_sequence[0][j])\n",
    "            val_sequence_norm[1][j]= scaler.transform(val_sequence[1][j].reshape(val_sequence[1][j].shape[0],1)).reshape(val_sequence[1][j].shape[0])\n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler.transform(test_sequence[0][j])\n",
    "            test_sequence_norm[1][j]= scaler.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "\n",
    "        x_train = train_sequence_norm[0]\n",
    "        y_train =train_sequence_norm[1]\n",
    "        x_val = val_sequence_norm[0]\n",
    "        y_val = val_sequence_norm[1]        \n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]             \n",
    "        \n",
    "        train_input = x_train\n",
    "        val_input = x_val\n",
    "        test_input = x_test\n",
    "\n",
    "####################################################################################\n",
    "#######creating train, validation and test sets from Predictions####################\n",
    "####################################################################################\n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-num_test]\n",
    "        \n",
    "        train_p = series_pred[:-n_val]                                        \n",
    "        val_p = series_pred[-(n_val+window_size):]\n",
    "        test_p = series_p[-(num_test+window_size):]\n",
    "\n",
    "        train_pred = make_k_input(train_p,window_size,horizon)\n",
    "        val_pred = make_k_input(val_p,window_size,horizon)\n",
    "        test_pred = nonov_make_k_input(test_p,window_size,horizon)        \n",
    "                \n",
    "        train_pred_norm=np.zeros(train_pred.shape)\n",
    "        val_pred_norm=np.zeros(val_pred.shape)\n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        scaler_pred = MinMaxScaler()\n",
    "        for j in range(train_pred.shape[0]):\n",
    "            scaler_pred.fit(train_pred[j])\n",
    "            train_pred_norm[j]= scaler_pred.transform(train_pred[j])\n",
    "        for j in range(val_pred.shape[0]):            \n",
    "            scaler_pred.fit(val_pred[j])\n",
    "            val_pred_norm[j]= scaler_pred.transform(val_pred[j])\n",
    "        for j in range(test_pred.shape[0]):            \n",
    "            scaler_pred.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler_pred.transform(test_pred[j])\n",
    "        \n",
    "        train_pred_norm=train_pred_norm.reshape(train_pred_norm.shape[0],train_pred_norm.shape[1])\n",
    "        val_pred_norm=val_pred_norm.reshape(val_pred_norm.shape[0],val_pred_norm.shape[1])\n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],test_pred_norm.shape[1])\n",
    "##########################################################################################################################\n",
    "        ncol = 56\n",
    "        encoding_dim = 32\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('Output/encoder_model_NN5_w70.h5')\n",
    "        train_pred_norm_auto=np.array(encoder.predict(train_pred_norm))\n",
    "        val_pred_norm_auto=np.array(encoder.predict(val_pred_norm))\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))        \n",
    "        \n",
    "        train_pred_norm_auto=pd.DataFrame(train_pred_norm_auto)\n",
    "        val_pred_norm_auto=pd.DataFrame(val_pred_norm_auto)\n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)                                    \n",
    "####################################################################################\n",
    "################       Neural Network Configuration         ########################\n",
    "####################################################################################\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        K.clear_session()\n",
    "        \n",
    "        input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        input_pred=Input(batch_shape=(None,encoding_dim),name='input_pred')\n",
    "\n",
    "        encoded_0=Reshape((1,32))(input_pred)\n",
    "        branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "        branch_0=concatenate([branch_0,encoded_0],axis=1)\n",
    "        branch_1 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_2 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "        branch_3=Flatten()(branch_2)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.00001))(branch_3)\n",
    "        \n",
    "        model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "        #callback = ModelCheckpoint(filepath='NetworkWeights_Checkpoints/checkpoint_NN5_02_70_%i.h5' %y,monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001))\n",
    "        #model.fit({'input_data':train_input,'input_pred':train_pred_norm_auto},y_train,validation_data=[[val_input,val_pred_norm_auto],y_val],callbacks=[callback],batch_size=12,shuffle=True, epochs=300,verbose=0)\n",
    "        model.load_weights('NetworkWeights_Checkpoints/checkpoint_NN5_02_70_%i.h5' %y)\n",
    "        \n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler.inverse_transform(pred)\n",
    "        \n",
    "        final_predictions[y,:num_test] = pred.reshape(num_test)\n",
    "        final_test[y,:num_test]=test[-num_test:]\n",
    "\n",
    "        AAA=smape(pred.reshape(num_test),current_test)\n",
    "        final_smape[y]=AAA\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('Output/prediction_NN5_02_70.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "model.summary()\n",
    "print('-----------------')\n",
    "print(\"SMAPE:\")\n",
    "print (sum(final_smape)/data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4: Only Data \n",
    "## The Network is trained by whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 253.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_data (InputLayer)      (None, 56, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 56, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 56, 64)            6208      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 56, 64)            12352     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 56)                200760    \n",
      "=================================================================\n",
      "Total params: 219,448\n",
      "Trainable params: 219,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-----------------\n",
      "SMAPE:\n",
      "21.690647686245676\n"
     ]
    }
   ],
   "source": [
    "data_length = data.shape[0]\n",
    "horizon=56\n",
    "window_size=56\n",
    "num_test=56\n",
    "for i in range(data_length): #data_length\n",
    "    current_row= np.asarray(data.iloc[i].dropna().values,dtype=float)\n",
    "    rr = current_row.size\n",
    "    rr = int(np.floor(rr*.25))\n",
    "    series_data = current_row[:-num_test]\n",
    "    series_length = series_data.size\n",
    "    n_val = int(np.round(series_length*.2))  \n",
    "    train = series_data[:-n_val]\n",
    "    val = series_data[-(n_val+window_size):]\n",
    "    train_sequence = make_input(train, window_size,horizon)\n",
    "    val_sequence = make_input(val,window_size,horizon)\n",
    "    \n",
    "    temp_train_x=train_sequence[0]\n",
    "    temp_train_x=temp_train_x.reshape(temp_train_x.shape[0],temp_train_x.shape[1])\n",
    "    temp_train_y=train_sequence[1]\n",
    "    \n",
    "    temp_val_x=val_sequence[0]\n",
    "    temp_val_x=temp_val_x.reshape(temp_val_x.shape[0],temp_val_x.shape[1])\n",
    "    temp_val_y=val_sequence[1]\n",
    "    if(i==0):\n",
    "        data_train_x=temp_train_x\n",
    "        data_train_y=temp_train_y\n",
    "        data_val_x=temp_val_x\n",
    "        data_val_y=temp_val_y\n",
    "    else:\n",
    "        data_train_x=np.concatenate((data_train_x,temp_train_x),axis=0)\n",
    "        data_train_y=np.concatenate((data_train_y,temp_train_y),axis=0)\n",
    "        data_val_x=np.concatenate((data_val_x,temp_val_x),axis=0)\n",
    "        data_val_y=np.concatenate((data_val_y,temp_val_y),axis=0)\n",
    "\n",
    "data_train_x=data_train_x.reshape(data_train_x.shape[0],data_train_x.shape[1],1)\n",
    "data_train_y=data_train_y.reshape(data_train_y.shape[0],data_train_y.shape[1],1)\n",
    "data_val_x=data_val_x.reshape(data_val_x.shape[0],data_val_x.shape[1],1)\n",
    "data_val_y=data_val_y.reshape(data_val_y.shape[0],data_val_y.shape[1],1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for j in range(data_train_x.shape[0]):\n",
    "    scaler.fit(data_train_x[j])\n",
    "    data_train_x[j]= scaler.transform(data_train_x[j])\n",
    "    data_train_y[j]= scaler.transform(data_train_y[j])\n",
    "for j in range(data_val_x.shape[0]):\n",
    "    scaler.fit(data_val_x[j])\n",
    "    data_val_x[j]= scaler.transform(data_val_x[j])\n",
    "    data_val_y[j]= scaler.transform(data_val_y[j])\n",
    "data_train_y=data_train_y.reshape(data_train_y.shape[0],data_train_y.shape[1])\n",
    "data_val_y=data_val_y.reshape(data_val_y.shape[0],data_val_y.shape[1])\n",
    "#########################################################################################\n",
    "#######################################################################################\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.reset_default_graph()\n",
    "K.clear_session()\n",
    "input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "        \n",
    "branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "branch_1 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "branch_2 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "#branch_3 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_2)\n",
    "#branch_4 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_3)\n",
    "\n",
    "branch_5=Flatten()(branch_2)\n",
    "\n",
    "net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.00001))(branch_5)\n",
    "\n",
    "model=Model(inputs=[input_data],outputs=net)\n",
    "callback = ModelCheckpoint(filepath='temp/checkpoint_NN5_01.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001))\n",
    "model.fit({'input_data':data_train_x},data_train_y,validation_data=[[data_val_x],data_val_y],callbacks=[callback],batch_size=16,shuffle=True, epochs=100,verbose=0)\n",
    "model.save('Output/trained_all_NN5.h5')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,56])\n",
    "    final_test = np.zeros([data_length,56])\n",
    "    final_smape=np.zeros(data_length)\n",
    "    for y in range(data_length):                \n",
    "        current_row =np.asarray(data.loc[y].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] \n",
    "        series_data = series_d[:-num_test]\n",
    "        series_length = series_data.size\n",
    "        current_test=series_d[-num_test:]\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "               \n",
    "        test = series_d[-(num_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler.transform(test_sequence[0][j])\n",
    "            test_sequence_norm[1][j]= scaler.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "            \n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]             \n",
    "\n",
    "        test_input = x_test\n",
    "\n",
    "        \n",
    "        pred=model.predict({'input_data':test_input})\n",
    "        pred=scaler.inverse_transform(pred)\n",
    "\n",
    "        final_predictions[y,:num_test] = pred.reshape(num_test)\n",
    "        final_test[y,:num_test]=test[-num_test:]\n",
    "\n",
    "        AAA=smape(pred.reshape(num_test),current_test)       \n",
    "        final_smape[y]=AAA\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('Output/prediction_NN5_01_wholedata.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "model.summary()\n",
    "print('-----------------')\n",
    "print(\"SMAPE:\")\n",
    "print (sum(final_smape)/data_length)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5: Data + Expert Knowledge\n",
    "## The Network is trained by whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1114 11:18:20.634135 11100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [01:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         (None, 56, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_pred (InputLayer)         (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 56, 32)       128         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 32)        0           input_pred[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 57, 32)       0           conv1d_1[0][0]                   \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 57, 64)       6208        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 57, 64)       12352       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3648)         0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_final (Dense)             (None, 56)           204344      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 223,032\n",
      "Trainable params: 223,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-----------------\n",
      "SMAPE:\n",
      "20.671226899359795\n"
     ]
    }
   ],
   "source": [
    "data_length = data.shape[0]\n",
    "horizon=56\n",
    "window_size=56\n",
    "num_test=56\n",
    "for i in range(data_length): #data_length\n",
    "    current_row= np.asarray(data.iloc[i].dropna().values,dtype=float)\n",
    "    rr = current_row.size\n",
    "    rr = int(np.floor(rr*.25))\n",
    "    series_d=current_row[rr:]\n",
    "    series_data = series_d[:-num_test]\n",
    "    series_length = series_data.size\n",
    "    n_val = int(np.round(series_length*.2))  \n",
    "    train = series_data[:-n_val]\n",
    "    val = series_data[-(n_val+window_size):]\n",
    "    train_sequence = make_input(train, window_size,horizon)\n",
    "    val_sequence = make_input(val,window_size,horizon)\n",
    "    \n",
    "    temp_train_x=train_sequence[0]\n",
    "    temp_train_x=temp_train_x.reshape(temp_train_x.shape[0],temp_train_x.shape[1])\n",
    "    temp_train_y=train_sequence[1]\n",
    "    \n",
    "    temp_val_x=val_sequence[0]\n",
    "    temp_val_x=temp_val_x.reshape(temp_val_x.shape[0],temp_val_x.shape[1])\n",
    "    temp_val_y=val_sequence[1]\n",
    "    if(i==0):\n",
    "        data_train_x=temp_train_x\n",
    "        data_train_y=temp_train_y\n",
    "        data_val_x=temp_val_x\n",
    "        data_val_y=temp_val_y\n",
    "    else:\n",
    "        data_train_x=np.concatenate((data_train_x,temp_train_x),axis=0)\n",
    "        data_train_y=np.concatenate((data_train_y,temp_train_y),axis=0)\n",
    "        data_val_x=np.concatenate((data_val_x,temp_val_x),axis=0)\n",
    "        data_val_y=np.concatenate((data_val_y,temp_val_y),axis=0)\n",
    "##################################\n",
    "##################################\n",
    "    current_pred= np.asarray(predictions.iloc[i].dropna().values,dtype=float)\n",
    "    series_p=current_pred\n",
    "    series_pred=series_p[:-num_test]    \n",
    "    train_p = series_pred[:-n_val]                                        \n",
    "    val_p = series_pred[-(n_val+window_size):]\n",
    "    train_pred = make_k_input(train_p,window_size,horizon)\n",
    "    val_pred = make_k_input(val_p,window_size,horizon)\n",
    "    \n",
    "    temp_train_p_x=train_pred\n",
    "    temp_train_p_x=temp_train_p_x.reshape(temp_train_p_x.shape[0],temp_train_p_x.shape[1])\n",
    "    \n",
    "    temp_val_p_x=val_pred\n",
    "    temp_val_p_x=temp_val_p_x.reshape(temp_val_p_x.shape[0],temp_val_p_x.shape[1])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    for j in range(temp_train_p_x.shape[0]):\n",
    "        scaler.fit(temp_train_p_x[j].reshape(temp_train_p_x[j].shape[0],1))\n",
    "        temp_train_p_x[j]= scaler.transform(temp_train_p_x[j].reshape(temp_train_p_x[j].shape[0],1)).reshape(temp_train_p_x[j].shape[0])\n",
    "    for j in range(temp_val_p_x.shape[0]):\n",
    "        scaler.fit(temp_val_p_x[j].reshape(temp_val_p_x[j].shape[0],1))\n",
    "        temp_val_p_x[j]= scaler.transform(temp_val_p_x[j].reshape(temp_val_p_x[j].shape[0],1)).reshape(temp_val_p_x[j].shape[0])\n",
    "    \n",
    "    ncol = 56\n",
    "    encoding_dim = 32\n",
    "    input_dim = Input(shape = (ncol, ))\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    input_dim = Input(shape = (ncol, ))\n",
    "    encoded_input = Input(shape = (encoding_dim, ))\n",
    "    encoder=load_model('Output/encoder_model_NN5_w70.h5')\n",
    "    \n",
    "    temp_train_p_x=np.array(encoder.predict(temp_train_p_x))\n",
    "    temp_val_p_x=np.array(encoder.predict(temp_val_p_x))\n",
    "    \n",
    "    if(i==0):\n",
    "        data_train_p_x=temp_train_p_x\n",
    "        data_val_p_x=temp_val_p_x\n",
    "    else:\n",
    "        data_train_p_x=np.concatenate((data_train_p_x,temp_train_p_x),axis=0)\n",
    "        data_val_p_x=np.concatenate((data_val_p_x,temp_val_p_x),axis=0)\n",
    "##################################\n",
    "##################################\n",
    "data_train_x=data_train_x.reshape(data_train_x.shape[0],data_train_x.shape[1],1)\n",
    "data_train_y=data_train_y.reshape(data_train_y.shape[0],data_train_y.shape[1],1)\n",
    "data_val_x=data_val_x.reshape(data_val_x.shape[0],data_val_x.shape[1],1)\n",
    "data_val_y=data_val_y.reshape(data_val_y.shape[0],data_val_y.shape[1],1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "for j in range(data_train_x.shape[0]):\n",
    "    scaler.fit(data_train_x[j])\n",
    "    data_train_x[j]= scaler.transform(data_train_x[j])\n",
    "    data_train_y[j]= scaler.transform(data_train_y[j])\n",
    "for j in range(data_val_x.shape[0]):\n",
    "    scaler.fit(data_val_x[j])\n",
    "    data_val_x[j]= scaler.transform(data_val_x[j])\n",
    "    data_val_y[j]= scaler.transform(data_val_y[j])\n",
    "data_train_y=data_train_y.reshape(data_train_y.shape[0],data_train_y.shape[1])\n",
    "data_val_y=data_val_y.reshape(data_val_y.shape[0],data_val_y.shape[1])\n",
    "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "#########################################################################################\n",
    "#######################################################################################\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.reset_default_graph()\n",
    "K.clear_session()\n",
    "input_data= Input(batch_shape=(None,window_size,1),name='input_data')\n",
    "input_pred=Input(batch_shape=(None,32),name='input_pred')\n",
    "\n",
    "encoded_0=Reshape((1,32))(input_pred)\n",
    "branch_0 = Conv1D(32,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(input_data)\n",
    "branch_0=concatenate([branch_0,encoded_0],axis=1)\n",
    "branch_1 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "branch_2 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_1)\n",
    "#branch_3 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_2)\n",
    "#branch_4 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_3)\n",
    "\n",
    "branch_5=Flatten()(branch_2)\n",
    "\n",
    "net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.00001))(branch_5)\n",
    "\n",
    "model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "callback = ModelCheckpoint(filepath='temp/checkpoint_NN5_02.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.00001))\n",
    "model.fit({'input_data':data_train_x,'input_pred':data_train_p_x},data_train_y,validation_data=[[data_val_x,data_val_p_x],data_val_y],callbacks=[callback],batch_size=16,shuffle=True, epochs=100,verbose=0)\n",
    "model.save('Output/trained_all_NN5_02.h5')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "with tqdm(total=data_length) as pbar:\n",
    "    final_predictions = np.zeros([data_length,56])\n",
    "    final_test = np.zeros([data_length,56])\n",
    "    final_smape=np.zeros(data_length)\n",
    "    for y in range(data_length):                \n",
    "        current_row =np.asarray(data.loc[y].dropna().values,dtype=float)\n",
    "        rr = current_row.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        series_d=current_row[rr:] \n",
    "        series_data = series_d[:-num_test]\n",
    "        series_length = series_data.size\n",
    "        current_test=series_d[-num_test:]\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "               \n",
    "        test = series_d[-(num_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "        test_sequence_norm = deepcopy(test_sequence)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        for j in range(test_sequence[0].shape[0]):\n",
    "            scaler.fit(test_sequence[0][j])\n",
    "            test_sequence_norm[0][j]= scaler.transform(test_sequence[0][j])\n",
    "            test_sequence_norm[1][j]= scaler.transform(test_sequence[1][j].reshape(test_sequence[1][j].shape[0],1)).reshape(test_sequence[1][j].shape[0])\n",
    "            \n",
    "        x_test = test_sequence_norm[0]\n",
    "        y_test = test_sequence_norm[1]             \n",
    "\n",
    "        test_input = x_test\n",
    "                        \n",
    "        \n",
    "        current_pred= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "        series_p=current_pred\n",
    "        series_pred=series_p[:-num_test]\n",
    "        \n",
    "        test_p = series_p[-(num_test+window_size):]\n",
    "        test_pred = nonov_make_k_input(test_p,window_size,horizon)        \n",
    "                \n",
    "        test_pred_norm=np.zeros(test_pred.shape)\n",
    "    \n",
    "        scaler_pred = MinMaxScaler()\n",
    "        for j in range(test_pred.shape[0]):            \n",
    "            scaler_pred.fit(test_pred[j])\n",
    "            test_pred_norm[j]= scaler_pred.transform(test_pred[j])\n",
    "        \n",
    "        test_pred_norm=test_pred_norm.reshape(test_pred_norm.shape[0],test_pred_norm.shape[1])\n",
    "##########################################################################################################################\n",
    "        ncol = 56\n",
    "        encoding_dim = 32\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "\n",
    "        input_dim = Input(shape = (ncol, ))\n",
    "        encoded_input = Input(shape = (encoding_dim, ))\n",
    "        encoder=load_model('Output/encoder_model_NN5_w70.h5')\n",
    "        test_pred_auto=np.array(encoder.predict(test_pred_norm))        \n",
    "        test_pred_auto=pd.DataFrame(test_pred_auto)    \n",
    "\n",
    "        \n",
    "        pred=model.predict({'input_data':test_input, 'input_pred':test_pred_auto})\n",
    "        pred=scaler.inverse_transform(pred)\n",
    "\n",
    "        final_predictions[y,:num_test] = pred.reshape(num_test)\n",
    "        final_test[y,:num_test]=test[-num_test:]\n",
    "\n",
    "        AAA=smape(pred.reshape(num_test),current_test)       \n",
    "        final_smape[y]=AAA\n",
    "        pbar.update(1)\n",
    "\n",
    "np.savetxt('Output/prediction_NN5_02_wholedata.csv',final_predictions, fmt='%1.3f',delimiter=',')\n",
    "model.summary()\n",
    "print('-----------------')\n",
    "print(\"SMAPE:\")\n",
    "print (sum(final_smape)/data_length)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
